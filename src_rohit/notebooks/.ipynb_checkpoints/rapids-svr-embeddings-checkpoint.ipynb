{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-12-01T12:20:37.960592Z",
     "iopub.status.busy": "2022-12-01T12:20:37.960186Z",
     "iopub.status.idle": "2022-12-01T12:20:37.965821Z",
     "shell.execute_reply": "2022-12-01T12:20:37.964627Z",
     "shell.execute_reply.started": "2022-12-01T12:20:37.960559Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os, gc, re, warnings \n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T12:20:37.972169Z",
     "iopub.status.busy": "2022-12-01T12:20:37.971352Z",
     "iopub.status.idle": "2022-12-01T12:20:38.070523Z",
     "shell.execute_reply": "2022-12-01T12:20:38.069506Z",
     "shell.execute_reply.started": "2022-12-01T12:20:37.972126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (3911, 9) Test shape: (3, 3) Test columns: Index(['text_id', 'full_text', 'src'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "      <th>src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>The best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>Small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
       "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n",
       "3  003885A45F42  The best time in life is when you become yours...       4.5   \n",
       "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions    src  \n",
       "0     3.5         3.0          3.0      4.0          3.0  train  \n",
       "1     2.5         3.0          2.0      2.0          2.5  train  \n",
       "2     3.5         3.0          3.0      3.0          2.5  train  \n",
       "3     4.5         4.5          4.5      4.0          5.0  train  \n",
       "4     3.0         3.0          3.0      2.5          2.5  train  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftr = pd.read_csv(\"../data/train.csv\")\n",
    "dftr[\"src\"]=\"train\"\n",
    "dfte = pd.read_csv(\"../data/test.csv\")\n",
    "dfte[\"src\"]=\"test\"\n",
    "print('Train shape:',dftr.shape,'Test shape:',dfte.shape,'Test columns:',dfte.columns)\n",
    "df = pd.concat([dftr,dfte],ignore_index=True)\n",
    "\n",
    "dftr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T12:20:38.074547Z",
     "iopub.status.busy": "2022-12-01T12:20:38.072930Z",
     "iopub.status.idle": "2022-12-01T12:20:38.080541Z",
     "shell.execute_reply": "2022-12-01T12:20:38.079394Z",
     "shell.execute_reply.started": "2022-12-01T12:20:38.074509Z"
    }
   },
   "outputs": [],
   "source": [
    "target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make 25 Stratified Folds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T12:20:38.082683Z",
     "iopub.status.busy": "2022-12-01T12:20:38.081910Z",
     "iopub.status.idle": "2022-12-01T12:20:38.258614Z",
     "shell.execute_reply": "2022-12-01T12:20:38.257564Z",
     "shell.execute_reply.started": "2022-12-01T12:20:38.082628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples per fold:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.0    157\n",
       "21.0    157\n",
       "1.0     157\n",
       "7.0     157\n",
       "20.0    157\n",
       "14.0    157\n",
       "19.0    157\n",
       "12.0    157\n",
       "23.0    157\n",
       "3.0     157\n",
       "6.0     157\n",
       "5.0     156\n",
       "13.0    156\n",
       "22.0    156\n",
       "0.0     156\n",
       "15.0    156\n",
       "24.0    156\n",
       "17.0    156\n",
       "9.0     156\n",
       "8.0     156\n",
       "4.0     156\n",
       "2.0     156\n",
       "18.0    156\n",
       "10.0    156\n",
       "16.0    156\n",
       "Name: FOLD, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append('../input/iterativestratification')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "FOLDS = 25\n",
    "skf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "for i,(train_index, val_index) in enumerate(skf.split(dftr,dftr[target_cols])):\n",
    "    dftr.loc[val_index,'FOLD'] = i\n",
    "print('Train samples per fold:')\n",
    "dftr.FOLD.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T12:20:38.260888Z",
     "iopub.status.busy": "2022-12-01T12:20:38.260229Z",
     "iopub.status.idle": "2022-12-01T12:20:38.266753Z",
     "shell.execute_reply": "2022-12-01T12:20:38.265711Z",
     "shell.execute_reply.started": "2022-12-01T12:20:38.260851Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel,AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T12:20:38.270832Z",
     "iopub.status.busy": "2022-12-01T12:20:38.270503Z",
     "iopub.status.idle": "2022-12-01T12:20:38.278083Z",
     "shell.execute_reply": "2022-12-01T12:20:38.276672Z",
     "shell.execute_reply.started": "2022-12-01T12:20:38.270806Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state.detach().cpu()\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T12:20:38.280081Z",
     "iopub.status.busy": "2022-12-01T12:20:38.279648Z",
     "iopub.status.idle": "2022-12-01T12:20:38.292247Z",
     "shell.execute_reply": "2022-12-01T12:20:38.291100Z",
     "shell.execute_reply.started": "2022-12-01T12:20:38.280048Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "class EmbedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self,idx):\n",
    "        text = self.df.loc[idx,\"full_text\"]\n",
    "        tokens = tokenizer(\n",
    "                text,\n",
    "                None,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=MAX_LEN,return_tensors=\"pt\")\n",
    "        tokens = {k:v.squeeze(0) for k,v in tokens.items()}\n",
    "        return tokens\n",
    "\n",
    "ds_tr = EmbedDataset(dftr)\n",
    "embed_dataloader_tr = torch.utils.data.DataLoader(ds_tr,\\\n",
    "                        batch_size=BATCH_SIZE,\\\n",
    "                        shuffle=False)\n",
    "ds_te = EmbedDataset(dfte)\n",
    "embed_dataloader_te = torch.utils.data.DataLoader(ds_te,\\\n",
    "                        batch_size=BATCH_SIZE,\\\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T12:20:38.294352Z",
     "iopub.status.busy": "2022-12-01T12:20:38.293966Z",
     "iopub.status.idle": "2022-12-01T12:20:38.320760Z",
     "shell.execute_reply": "2022-12-01T12:20:38.319652Z",
     "shell.execute_reply.started": "2022-12-01T12:20:38.294315Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = None\n",
    "MAX_LEN = 512\n",
    "\n",
    "def get_embeddings(MODEL_NM='', TOK_NM = '', MAX=512, BATCH_SIZE=4, verbose=True):\n",
    "    global tokenizer, MAX_LEN\n",
    "    DEVICE=\"cuda\"\n",
    "    model = AutoModel.from_pretrained( MODEL_NM )\n",
    "    tokenizer = AutoTokenizer.from_pretrained( MODEL_NM )\n",
    "    MAX_LEN = MAX\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    all_train_text_feats = []\n",
    "    for batch in tqdm(embed_dataloader_tr,total=len(embed_dataloader_tr)):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n",
    "        # Normalize the embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "        all_train_text_feats.extend(sentence_embeddings)\n",
    "    all_train_text_feats = np.array(all_train_text_feats)\n",
    "    if verbose:\n",
    "        print('Train embeddings shape',all_train_text_feats.shape)\n",
    "        \n",
    "    te_text_feats = []\n",
    "    for batch in tqdm(embed_dataloader_te,total=len(embed_dataloader_te)):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n",
    "        # Normalize the embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "        te_text_feats.extend(sentence_embeddings)\n",
    "    te_text_feats = np.array(te_text_feats)\n",
    "    if verbose:\n",
    "        print('Test embeddings shape',te_text_feats.shape)\n",
    "        \n",
    "    return all_train_text_feats, te_text_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Base Embeddings all-roberta-large-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T12:20:38.322815Z",
     "iopub.status.busy": "2022-12-01T12:20:38.322256Z",
     "iopub.status.idle": "2022-12-01T12:26:50.431802Z",
     "shell.execute_reply": "2022-12-01T12:26:50.430749Z",
     "shell.execute_reply.started": "2022-12-01T12:20:38.322772Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████▊                                             | 144/978 [00:11<01:09, 12.06it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m MODEL_NM \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m all_train_text_feats, te_text_feats \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NM\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfb_bart_large.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(f, all_train_text_feats)\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(MODEL_NM, TOK_NM, MAX, BATCH_SIZE, verbose)\u001b[0m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     13\u001b[0m all_train_text_feats \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(embed_dataloader_tr,total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(embed_dataloader_tr)):\n\u001b[1;32m     15\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     16\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[0;32m~/ml/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/ml/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/ml/lib/python3.8/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/ml/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/ml/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mEmbedDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m,idx):\n\u001b[1;32m      9\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mloc[idx,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_LEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m {k:v\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[0;32m~/ml/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2484\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2483\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2484\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/ml/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2590\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2571\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2572\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2587\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2593\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2608\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2609\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ml/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2663\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2655\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2656\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2660\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2661\u001b[0m )\n\u001b[0;32m-> 2663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2666\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2681\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2682\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ml/lib/python3.8/site-packages/transformers/models/bart/tokenization_bart_fast.py:270\u001b[0m, in \u001b[0;36mBartTokenizerFast._encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_split_into_words \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space:\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m     )\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ml/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:500\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    479\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    497\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    499\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 500\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/ml/lib/python3.8/site-packages/transformers/models/bart/tokenization_bart_fast.py:259\u001b[0m, in \u001b[0;36mBartTokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_split_into_words \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m     )\n\u001b[0;32m--> 259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ml/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:427\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    420\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    421\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    424\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    425\u001b[0m )\n\u001b[0;32m--> 427\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    439\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    441\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    451\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MODEL_NM = \"facebook/bart-large\"\n",
    "all_train_text_feats, te_text_feats = get_embeddings(MODEL_NM) \n",
    "\n",
    "with open(f\"../MODEL_DIR/embeddings/fb_bart_large.npy\", 'wb') as f:\n",
    "    np.save(f, all_train_text_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T12:26:50.434128Z",
     "iopub.status.busy": "2022-12-01T12:26:50.433470Z",
     "iopub.status.idle": "2022-12-01T12:26:50.635147Z",
     "shell.execute_reply": "2022-12-01T12:26:50.634045Z",
     "shell.execute_reply.started": "2022-12-01T12:26:50.434089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our concatenated embeddings have shape (3911, 1024)\n"
     ]
    }
   ],
   "source": [
    "all_train_text_feats = np.concatenate([all_train_text_feats,],axis=1)\n",
    "te_text_feats = np.concatenate([te_text_feats,],axis=1)\n",
    "gc.collect()\n",
    "\n",
    "print('Our concatenated embeddings have shape', all_train_text_feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RAPIDS cuML SVR\n",
    "Documentation for RAPIDS SVM is [here][1]\n",
    "\n",
    "[1]: https://docs.rapids.ai/api/cuml/stable/api.html#support-vector-machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T12:26:50.637190Z",
     "iopub.status.busy": "2022-12-01T12:26:50.636828Z",
     "iopub.status.idle": "2022-12-01T12:26:50.642863Z",
     "shell.execute_reply": "2022-12-01T12:26:50.641855Z",
     "shell.execute_reply.started": "2022-12-01T12:26:50.637154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAPIDS version 21.10.02\n"
     ]
    }
   ],
   "source": [
    "from cuml.svm import SVR\n",
    "import cuml\n",
    "print('RAPIDS version',cuml.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T12:26:50.644926Z",
     "iopub.status.busy": "2022-12-01T12:26:50.644255Z",
     "iopub.status.idle": "2022-12-01T12:27:07.426524Z",
     "shell.execute_reply": "2022-12-01T12:27:07.425403Z",
     "shell.execute_reply.started": "2022-12-01T12:26:50.644882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 0 RSME score: 0.4674734393036422\n",
      "#########################\n",
      "### Fold 2\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 1 RSME score: 0.45977760882071594\n",
      "#########################\n",
      "### Fold 3\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 2 RSME score: 0.47532701883274625\n",
      "#########################\n",
      "### Fold 4\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 3 RSME score: 0.4578513058101757\n",
      "#########################\n",
      "### Fold 5\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 4 RSME score: 0.45620286581210984\n",
      "#########################\n",
      "### Fold 6\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 5 RSME score: 0.4423127144695053\n",
      "#########################\n",
      "### Fold 7\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 6 RSME score: 0.4571818283260683\n",
      "#########################\n",
      "### Fold 8\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 7 RSME score: 0.4715255532062801\n",
      "#########################\n",
      "### Fold 9\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 8 RSME score: 0.45464396264626633\n",
      "#########################\n",
      "### Fold 10\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 9 RSME score: 0.5030198786945187\n",
      "#########################\n",
      "### Fold 11\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 10 RSME score: 0.45595736053905206\n",
      "#########################\n",
      "### Fold 12\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 11 RSME score: 0.461756133394835\n",
      "#########################\n",
      "### Fold 13\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 12 RSME score: 0.46693445347604606\n",
      "#########################\n",
      "### Fold 14\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 13 RSME score: 0.4407422120939262\n",
      "#########################\n",
      "### Fold 15\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 14 RSME score: 0.4748914743565302\n",
      "#########################\n",
      "### Fold 16\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 15 RSME score: 0.4436707871691208\n",
      "#########################\n",
      "### Fold 17\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 16 RSME score: 0.43724964388692317\n",
      "#########################\n",
      "### Fold 18\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 17 RSME score: 0.4686714040153799\n",
      "#########################\n",
      "### Fold 19\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 18 RSME score: 0.46695666215983583\n",
      "#########################\n",
      "### Fold 20\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 19 RSME score: 0.44865645731164006\n",
      "#########################\n",
      "### Fold 21\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 20 RSME score: 0.4877803140300838\n",
      "#########################\n",
      "### Fold 22\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 21 RSME score: 0.48205199090041867\n",
      "#########################\n",
      "### Fold 23\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 22 RSME score: 0.46248262407057367\n",
      "#########################\n",
      "### Fold 24\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 23 RSME score: 0.4795484289145891\n",
      "#########################\n",
      "### Fold 25\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 24 RSME score: 0.4514777802485119\n",
      "#########################\n",
      "Overall CV RSME = 0.46296575609957974\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = []\n",
    "scores = []\n",
    "def comp_score(y_true,y_pred):\n",
    "    rmse_scores = []\n",
    "    for i in range(len(target_cols)):\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_true[:,i],y_pred[:,i])))\n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "\n",
    "oof_df = pd.DataFrame()\n",
    "#for fold in tqdm(range(FOLDS),total=FOLDS):\n",
    "for fold in range(FOLDS):\n",
    "    print('#'*25) \n",
    "    print('### Fold',fold+1) \n",
    "    print('#'*25) \n",
    "    \n",
    "    dftr_ = dftr[dftr[\"FOLD\"]!=fold] \n",
    "    dfev_ = dftr[dftr[\"FOLD\"]==fold] \n",
    "    \n",
    "    tr_text_feats = all_train_text_feats[list(dftr_.index),:]\n",
    "    ev_text_feats = all_train_text_feats[list(dfev_.index),:]\n",
    "    \n",
    "    ev_preds = np.zeros((len(ev_text_feats),6))\n",
    "    test_preds = np.zeros((len(te_text_feats),6))\n",
    "    for i,t in enumerate(target_cols):\n",
    "        print(t,', ',end='')\n",
    "        clf = SVR(C=1)\n",
    "        clf.fit(tr_text_feats, dftr_[t].values)\n",
    "        ev_preds[:,i] = clf.predict(ev_text_feats)\n",
    "        test_preds[:,i] = clf.predict(te_text_feats)\n",
    "    print() \n",
    "    score = comp_score(dfev_[target_cols].values,ev_preds)\n",
    "    scores.append(score)\n",
    "    print(\"Fold : {} RSME score: {}\".format(fold,score))\n",
    "    \n",
    "    dfev_[[f\"pred_{c}\" for c in target_cols]] = ev_preds\n",
    "    oof_df = pd.concat([oof_df, dfev_])\n",
    "    preds.append(test_preds)\n",
    "    \n",
    "oof_df = oof_df.reset_index(drop=True)\n",
    "oof_df.to_csv(f\"fb_bart_large_512_oof.csv\")    \n",
    "\n",
    "print('#'*25)\n",
    "print('Overall CV RSME =',np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Base Embeddings flax-sentence-embeddings/all_datasets_v3_roberta-large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T12:27:07.428634Z",
     "iopub.status.busy": "2022-12-01T12:27:07.428249Z",
     "iopub.status.idle": "2022-12-01T12:32:46.885157Z",
     "shell.execute_reply": "2022-12-01T12:32:46.884284Z",
     "shell.execute_reply.started": "2022-12-01T12:27:07.428597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef8a32d86514b0d8a02a100a8f9526e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/650 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd74bd4791b4c38adcb6fa498521809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.32G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d02427c41a24045ba402d4082c20d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/328 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8105df673b4f42538a442281649e8cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/780k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659e112e5f254d9d880bd42cc0107579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de0621cceca404aa192020a54a43076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196c8ae466914232bf1b66176a2322cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 978/978 [04:22<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape (3911, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test embeddings shape (3, 1024)\n"
     ]
    }
   ],
   "source": [
    "MODEL_NM = \"flax-sentence-embeddings/all_datasets_v3_roberta-large\"\n",
    "all_train_text_feats, te_text_feats = get_embeddings(MODEL_NM) \n",
    "\n",
    "with open(f\"ad_v3_roberta_large.npy\", 'wb') as f:\n",
    "    np.save(f, all_train_text_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T12:32:46.887281Z",
     "iopub.status.busy": "2022-12-01T12:32:46.886821Z",
     "iopub.status.idle": "2022-12-01T12:32:47.082938Z",
     "shell.execute_reply": "2022-12-01T12:32:47.081894Z",
     "shell.execute_reply.started": "2022-12-01T12:32:46.887245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our concatenated embeddings have shape (3911, 1024)\n"
     ]
    }
   ],
   "source": [
    "all_train_text_feats = np.concatenate([all_train_text_feats,],axis=1)\n",
    "te_text_feats = np.concatenate([te_text_feats,],axis=1)\n",
    "gc.collect()\n",
    "\n",
    "print('Our concatenated embeddings have shape', all_train_text_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T12:32:47.087052Z",
     "iopub.status.busy": "2022-12-01T12:32:47.086732Z",
     "iopub.status.idle": "2022-12-01T12:33:08.017656Z",
     "shell.execute_reply": "2022-12-01T12:33:08.016107Z",
     "shell.execute_reply.started": "2022-12-01T12:32:47.087022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 0 RSME score: 0.49740754752263183\n",
      "#########################\n",
      "### Fold 2\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 1 RSME score: 0.4986975988622538\n",
      "#########################\n",
      "### Fold 3\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 2 RSME score: 0.5163395057448988\n",
      "#########################\n",
      "### Fold 4\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 3 RSME score: 0.4933252589668251\n",
      "#########################\n",
      "### Fold 5\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 4 RSME score: 0.5139417091124985\n",
      "#########################\n",
      "### Fold 6\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 5 RSME score: 0.4871673543463064\n",
      "#########################\n",
      "### Fold 7\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 6 RSME score: 0.4911479288662519\n",
      "#########################\n",
      "### Fold 8\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 7 RSME score: 0.49147132825323575\n",
      "#########################\n",
      "### Fold 9\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 8 RSME score: 0.48880131608844546\n",
      "#########################\n",
      "### Fold 10\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 9 RSME score: 0.5768584381516458\n",
      "#########################\n",
      "### Fold 11\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 10 RSME score: 0.5020559230085732\n",
      "#########################\n",
      "### Fold 12\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 11 RSME score: 0.49514350057420864\n",
      "#########################\n",
      "### Fold 13\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 12 RSME score: 0.5172916405344031\n",
      "#########################\n",
      "### Fold 14\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 13 RSME score: 0.5011000282477155\n",
      "#########################\n",
      "### Fold 15\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 14 RSME score: 0.5486509606451452\n",
      "#########################\n",
      "### Fold 16\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 15 RSME score: 0.49714037509393205\n",
      "#########################\n",
      "### Fold 17\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 16 RSME score: 0.49103628423065243\n",
      "#########################\n",
      "### Fold 18\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 17 RSME score: 0.5045328946140358\n",
      "#########################\n",
      "### Fold 19\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 18 RSME score: 0.5268363900647356\n",
      "#########################\n",
      "### Fold 20\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 19 RSME score: 0.5080171641327763\n",
      "#########################\n",
      "### Fold 21\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 20 RSME score: 0.5334352226017024\n",
      "#########################\n",
      "### Fold 22\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 21 RSME score: 0.5173754036412589\n",
      "#########################\n",
      "### Fold 23\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 22 RSME score: 0.5101498468499908\n",
      "#########################\n",
      "### Fold 24\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 23 RSME score: 0.5420356771240809\n",
      "#########################\n",
      "### Fold 25\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 24 RSME score: 0.5123350044642372\n",
      "#########################\n",
      "Overall CV RSME = 0.5104917720696978\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = []\n",
    "scores = []\n",
    "def comp_score(y_true,y_pred):\n",
    "    rmse_scores = []\n",
    "    for i in range(len(target_cols)):\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_true[:,i],y_pred[:,i])))\n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "\n",
    "oof_df = pd.DataFrame()\n",
    "#for fold in tqdm(range(FOLDS),total=FOLDS):\n",
    "for fold in range(FOLDS):\n",
    "    print('#'*25) \n",
    "    print('### Fold',fold+1) \n",
    "    print('#'*25) \n",
    "    \n",
    "    dftr_ = dftr[dftr[\"FOLD\"]!=fold] \n",
    "    dfev_ = dftr[dftr[\"FOLD\"]==fold] \n",
    "    \n",
    "    tr_text_feats = all_train_text_feats[list(dftr_.index),:]\n",
    "    ev_text_feats = all_train_text_feats[list(dfev_.index),:]\n",
    "    \n",
    "    ev_preds = np.zeros((len(ev_text_feats),6))\n",
    "    test_preds = np.zeros((len(te_text_feats),6))\n",
    "    for i,t in enumerate(target_cols):\n",
    "        print(t,', ',end='')\n",
    "        clf = SVR(C=1)\n",
    "        clf.fit(tr_text_feats, dftr_[t].values)\n",
    "        ev_preds[:,i] = clf.predict(ev_text_feats)\n",
    "        test_preds[:,i] = clf.predict(te_text_feats)\n",
    "    print() \n",
    "    score = comp_score(dfev_[target_cols].values,ev_preds)\n",
    "    scores.append(score)\n",
    "    print(\"Fold : {} RSME score: {}\".format(fold,score))\n",
    "    \n",
    "    dfev_[[f\"pred_{c}\" for c in target_cols]] = ev_preds\n",
    "    oof_df = pd.concat([oof_df, dfev_])\n",
    "    preds.append(test_preds)\n",
    "    \n",
    "oof_df = oof_df.reset_index(drop=True)\n",
    "oof_df.to_csv(f\"ad_v3_roberta_large.csv\")    \n",
    "\n",
    "print('#'*25)\n",
    "print('Overall CV RSME =',np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Base Embeddings facebook/bart-large-mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T12:33:08.019628Z",
     "iopub.status.busy": "2022-12-01T12:33:08.019174Z",
     "iopub.status.idle": "2022-12-01T12:37:37.426188Z",
     "shell.execute_reply": "2022-12-01T12:37:37.425264Z",
     "shell.execute_reply.started": "2022-12-01T12:33:08.019589Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 978/978 [04:21<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape (3911, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test embeddings shape (3, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NM = \"flax-sentence-embeddings/all_datasets_v3_roberta-large\"\n",
    "all_train_text_feats, te_text_feats = get_embeddings(MODEL_NM) \n",
    "\n",
    "with open(f\"fb_bart_large_mnli.npy\", 'wb') as f:\n",
    "    np.save(f, all_train_text_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T12:37:37.428705Z",
     "iopub.status.busy": "2022-12-01T12:37:37.427704Z",
     "iopub.status.idle": "2022-12-01T12:37:37.616851Z",
     "shell.execute_reply": "2022-12-01T12:37:37.615958Z",
     "shell.execute_reply.started": "2022-12-01T12:37:37.428664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our concatenated embeddings have shape (3911, 1024)\n"
     ]
    }
   ],
   "source": [
    "all_train_text_feats = np.concatenate([all_train_text_feats,],axis=1)\n",
    "te_text_feats = np.concatenate([te_text_feats,],axis=1)\n",
    "gc.collect()\n",
    "\n",
    "print('Our concatenated embeddings have shape', all_train_text_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T12:37:37.619989Z",
     "iopub.status.busy": "2022-12-01T12:37:37.619375Z",
     "iopub.status.idle": "2022-12-01T12:37:58.344164Z",
     "shell.execute_reply": "2022-12-01T12:37:58.342434Z",
     "shell.execute_reply.started": "2022-12-01T12:37:37.619950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 0 RSME score: 0.49740754752263183\n",
      "#########################\n",
      "### Fold 2\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 1 RSME score: 0.4986975988622538\n",
      "#########################\n",
      "### Fold 3\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 2 RSME score: 0.5163395057448988\n",
      "#########################\n",
      "### Fold 4\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 3 RSME score: 0.4933252589668251\n",
      "#########################\n",
      "### Fold 5\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 4 RSME score: 0.5139417091124985\n",
      "#########################\n",
      "### Fold 6\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 5 RSME score: 0.4871673543463064\n",
      "#########################\n",
      "### Fold 7\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 6 RSME score: 0.4911479288662519\n",
      "#########################\n",
      "### Fold 8\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 7 RSME score: 0.49147132825323575\n",
      "#########################\n",
      "### Fold 9\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 8 RSME score: 0.48880131608844546\n",
      "#########################\n",
      "### Fold 10\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 9 RSME score: 0.5768584381516458\n",
      "#########################\n",
      "### Fold 11\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 10 RSME score: 0.5020559230085732\n",
      "#########################\n",
      "### Fold 12\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 11 RSME score: 0.49514350057420864\n",
      "#########################\n",
      "### Fold 13\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 12 RSME score: 0.5172916405344031\n",
      "#########################\n",
      "### Fold 14\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 13 RSME score: 0.5011000282477155\n",
      "#########################\n",
      "### Fold 15\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 14 RSME score: 0.5486509606451452\n",
      "#########################\n",
      "### Fold 16\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 15 RSME score: 0.49714037509393205\n",
      "#########################\n",
      "### Fold 17\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 16 RSME score: 0.49103628423065243\n",
      "#########################\n",
      "### Fold 18\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 17 RSME score: 0.5045328946140358\n",
      "#########################\n",
      "### Fold 19\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 18 RSME score: 0.5268363900647356\n",
      "#########################\n",
      "### Fold 20\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 19 RSME score: 0.5080171641327763\n",
      "#########################\n",
      "### Fold 21\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 20 RSME score: 0.5334352226017024\n",
      "#########################\n",
      "### Fold 22\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 21 RSME score: 0.5173754036412589\n",
      "#########################\n",
      "### Fold 23\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 22 RSME score: 0.5101498468499908\n",
      "#########################\n",
      "### Fold 24\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 23 RSME score: 0.5420356771240809\n",
      "#########################\n",
      "### Fold 25\n",
      "#########################\n",
      "cohesion , syntax , vocabulary , phraseology , grammar , conventions , \n",
      "Fold : 24 RSME score: 0.5123350044642372\n",
      "#########################\n",
      "Overall CV RSME = 0.5104917720696978\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = []\n",
    "scores = []\n",
    "def comp_score(y_true,y_pred):\n",
    "    rmse_scores = []\n",
    "    for i in range(len(target_cols)):\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_true[:,i],y_pred[:,i])))\n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "\n",
    "oof_df = pd.DataFrame()\n",
    "#for fold in tqdm(range(FOLDS),total=FOLDS):\n",
    "for fold in range(FOLDS):\n",
    "    print('#'*25) \n",
    "    print('### Fold',fold+1) \n",
    "    print('#'*25) \n",
    "    \n",
    "    dftr_ = dftr[dftr[\"FOLD\"]!=fold] \n",
    "    dfev_ = dftr[dftr[\"FOLD\"]==fold] \n",
    "    \n",
    "    tr_text_feats = all_train_text_feats[list(dftr_.index),:]\n",
    "    ev_text_feats = all_train_text_feats[list(dfev_.index),:]\n",
    "    \n",
    "    ev_preds = np.zeros((len(ev_text_feats),6))\n",
    "    test_preds = np.zeros((len(te_text_feats),6))\n",
    "    for i,t in enumerate(target_cols):\n",
    "        print(t,', ',end='')\n",
    "        clf = SVR(C=1)\n",
    "        clf.fit(tr_text_feats, dftr_[t].values)\n",
    "        ev_preds[:,i] = clf.predict(ev_text_feats)\n",
    "        test_preds[:,i] = clf.predict(te_text_feats)\n",
    "    print() \n",
    "    score = comp_score(dfev_[target_cols].values,ev_preds)\n",
    "    scores.append(score)\n",
    "    print(\"Fold : {} RSME score: {}\".format(fold,score))\n",
    "    \n",
    "    dfev_[[f\"pred_{c}\" for c in target_cols]] = ev_preds\n",
    "    oof_df = pd.concat([oof_df, dfev_])\n",
    "    preds.append(test_preds)\n",
    "    \n",
    "oof_df = oof_df.reset_index(drop=True)\n",
    "oof_df.to_csv(f\"fb_bart_large_mnli.csv\")    \n",
    "\n",
    "print('#'*25)\n",
    "print('Overall CV RSME =',np.mean(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
